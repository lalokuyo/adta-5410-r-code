---
title: "ADTA 5410: Week 7 R Lab"
author: "Eduardo Banuelos"
format: html
editor: visual
---

## Data

We will use North Carololina birth records for the year 2004. We want to look at the relation between the habits and practices of expectant mothers and the birth of their children. The dataset is a random sample from the original dataset.

## 

**Predictors**

-   **fage**: father's age in years.

-   **mage**: mother's age in years.

-   **mature**: maturity status of mother.

-   **weeks**: length of pregnancy in weeks.

-   **premie**: whether the birth was classified as premature (premie) or full-term.

-   **visits**: number of hospital visits during pregnancy.

-   **marital**: whether mother is married or not married at birth.

-   **gained**: weight gained by mother during pregnancy in pounds.

-   **gender**: gender of the baby, female or male.

-   **habit**: status of the mother as a nonsmoker or a smoker.

-   **whitemom**: whether mother is white or not white.

# **Outcome variables[Â¶](https://instructorecdsjwfd.labs.coursera.org/notebooks/source/Week5/Week5_Lab.ipynb#Outcome-variables)**

-   **weight**: weight of the baby at birth in pounds. (Regression problem)

```{r}
library(dplyr)
library(ggplot2)
library(rpart)


# data is stored in a csv file, the first row contains the variable names. 
# we call our data mydata
mydata<-read.csv ("nc.csv", header=TRUE)
# let's print the data structure to have an idea of the data we are dealing with

mydata<-mydata%>%
  select(-lowbirthweight)
str(mydata)

```

```{r}
# Let's also take a look at the data
print(head(mydata))

# check the summary statistics 

print(summary(mydata))
```

## 

Data Split

Before conducting our analysis, we will split our data into two, one for training and one for testing. In this lab assignment, use the **sample** function in **rsample** package to split your data by using the following seed: **set.seed(1234)**

# **Task 1:**

Bu using the **sample** function in R, split **mydata** into training and test sets by putting 70% of the data in training. Use set.seed(1234) when you do the split. Name the training set as **train_data** and the test set as **test_data**.

```{r}
# split the sample
# Using rsample package
library(rsample)

set.seed(1234)
# randomly select 70% of the rows for training
train_index <- sample(nrow(mydata), 0.7 * nrow(mydata)) 
# create training dataset
train_data <- mydata[train_index, ] 
# create test dataset by removing the training rows from the original dataset
test_data <- mydata[-train_index, ] 



```

## Task 2:

In this task, you will be using the **`train_data`** dataset to run a linear regression that takes 'weight' as the dependent variable and all the other columns as the predictor.

-   You will use the **`lm()`** function to estimate your linear model and name it as **`linearmodel`**.

-   Use the **`predict()`** function to predict the 'weight' variable in the **`test_data`** dataset using **`linearmodel`**.

-   Store the resulting predictions in a new object called **`predicted_weights`**.

-   Calculate the mean squared prediction error in the **`test_data`** dataset by comparing the predicted 'weight' values with the actual 'weight' values. Store the resulting value in an object called **`MSPE_linear`**.

-   Print the value of **`MSPE_linear`** to the console using the **`print()`** function.

    ```{r, echo=TRUE}

    set.seed(1234)

    # Fiting the linear model
    linearmodel <- lm(weight ~ . , data = train_data)
    #linearmodel

    # Predict the future values
    predicted_weights <- predict(linearmodel, newdata = test_data)

    # Calculate the MSE
    MSPE_linear <- mean((predicted_weights - test_data$weight)^2, na.rm = TRUE)
    print(MSPE_linear)
    ```

    ## 

    Task3

-   Use the generalized additive model (GAM) function in the **`mgcv`** package to complete the same task. In other words, fit a GAM on the `train_data` using the **`gam()`** function. Use the **`s()`** function for each predictor. By doing so, you specify that each predictor variable is modeled using a smoothing spline. Save your R object as `gam_model`

-   Use the **`predict()`** function to predict the 'weight' variable in the **`test_data`** dataset using **`gam_model`**. Store the resulting predictions in a new object called **`predicted_weights`**.

-   Calculate the mean squared prediction error in the **`test_data`** dataset by comparing the predicted 'weight' values with the actual 'weight' values. Store the resulting value in an object called **`MSPE_gam`**.

Print the value of **`MSPE_gam`** to the console using the **`print()`** function.

```{r, echo=TRUE}
library(mgcv)

set.seed(1234)

#str(train_data)
# Fit a GAM model with train_data
gam_model <- gam(weight ~ s(fage) + s(mage) + s(weeks) + s(visits) + s(gained), data = train_data)
#gam_model

# Predict the future values
predicted_weights <- predict(gam_model, newdata = test_data)

# Calculate the MSE
MSPE_gam <- mean((predicted_weights - test_data$weight)^2, na.rm = TRUE)
print(MSPE_gam)
```

## Task 4

-   Compare the mean squared prediction error obtained from the linear regression model (**`linearmodel`**) in Task 2 and the generalized additive model (**`gam_model`**) in the previous task. You will use the **`MSPE_linear`** and **`MSPE_gam`** values to determine which model performs better in predicting the 'weight' variable in the **`test_data`** dataset.

```{r,echo=TRUE}

# Compare MSPE values
if(MSPE_linear < MSPE_gam){
  print("Linear regression model performs better than GAM.")
}else if(MSPE_linear > MSPE_gam){
  print("GAM model performs better than linear regression.")
}else{
  print("Both models have similar performance.")
}
```

-   Comment on your findings.

Task 3: The model with the lower MSPE value is considered better.
